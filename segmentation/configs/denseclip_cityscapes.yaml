# configs/denseclip_cityscapes.yaml

# Data configuration
data:
  dataset_type: 'CityscapesDepthSegDataset' # <<< CHANGE: Use the new dataset class
  path: 'data/cityscapes'                 # <<< VERIFY: Path containing leftImg8bit, gtFine, disparity, camera dirs
  classes: 19                              # Number of segmentation classes (used by dataset/metrics)
  ignore_label: 255                        # Ignore index for segmentation
  depth_max: 80.0                          # Optional: Max depth value to consider valid in dataset (meters)
  crop_size: [512, 1024]                   # Target crop size for training/validation
  scale_range: [0.5, 2.0]                  # Random scaling range for training augmentation
  norm_mean: [0.48145466, 0.4578275, 0.40821073] # CLIP default normalization
  norm_std: [0.26862954, 0.26130258, 0.27577711]  # CLIP default normalization
  # samples_per_gpu / workers_per_gpu are not used by the script, use training section below

# Model configuration
model:
  type: 'DenseCLIP'
  clip_pretrained: '/home/22dcs005/DenseCLIP/segmentation/pretrained/ViT-B-16.pt' # <<< VERIFY: Correct absolute path to ViT weights
  text_dim: 512             # <<< Target shared dimension (matches standard CLIP Text output)
  context_length: 6           # <<< Length for FIXED class name tokens passed to tokenizer
  token_embed_dim: 512      # <<< Dimension of the learnable context vectors (usually matches text encoder width)
  context_feature: 'attention'
  score_concat_index: -1      # Disable score map concatenation for ViT
  text_head: False
  tau: 0.05

  backbone:
    type: 'CLIPVisionTransformer'
    patch_size: 16
    width: 768                # ViT-B/16 internal dimension
    layers: 12
    heads: 12
    input_resolution: 224     # Standard CLIP ViT input size
    output_dim: 768           # Output internal dim (pre-projection) from backbone class
    out_indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] # Output from ALL 12 blocks

  text_encoder:
    type: 'CLIPTextContextEncoder' # Using the context encoder
    context_length: 22         # <<< Total internal capacity (fixed text + learnable context)
    vocab_size: 49408
    transformer_width: 512    # Internal width of text transformer
    transformer_heads: 8
    transformer_layers: 12
    embed_dim: 512            # <<< Final output dimension (matches text_dim)

  # context_decoder: None # Remove or comment out if not used

  neck:
    type: 'ViTFeatureFusionNeck' # Specify the custom neck
    # in_channels_list: Automatically determined by code based on backbone config
    inter_channels: 128       # Optional: Internal processing channels
    out_channels: 256         # Final output dimension of the neck

  decode_head: # Segmentation Head
    type: 'FPNHead'           # Uses torchvision FCNHead internally
    in_channels: 256          # <<< MUST match neck.out_channels
    channels: 256             # Internal channels for the head's convs
    num_classes: 19           # Number of segmentation classes
    align_corners: False
    dropout_ratio: 0.1        # Dropout within FCNHead layers

  # --- VVVVV ADD DEPTH HEAD CONFIG VVVVV ---
  depth_head:
    type: 'FCNHeadDepth'      # Type name matching implementation in DenseCLIP.__init__
    in_channels: 256          # <<< MUST match neck.out_channels (takes same input as seg head)
    channels: 128             # Internal channels (can be different from seg head)
    # num_classes: 1         # Implicitly 1 for depth head
    align_corners: False      # Use same setting as seg head or choose appropriately
    # dropout_ratio: 0.1      # Optional dropout for depth head
  # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

  # auxiliary_head: None # Remove or comment out
  # identity_head: None  # Remove or comment out

# Training configuration
training:
  epochs: 100
  batch_size: 8             # Keep reduced for ViT
  workers: 8                # Keep reduced
  optimizer:
    type: 'AdamW'
    lr: 2.0e-05             # Keep reduced LR for ViT fine-tuning
    weight_decay: 0.01
  scheduler: # Example scheduler config (kept from previous)
    type: CosineAnnealingLR
    T_max: 100                # Match epochs
    eta_min: 1.0e-06
  # min_lr: 1.0e-06           # Included in scheduler eta_min above
  # log_interval: 50          # Not used by current script logging logic
  seed: 42
  eval_interval: 1
  save_interval: 5
  grad_accum_steps: 1       # Set > 1 to simulate larger batches if needed
  clip_grad_norm: null        # Set to a float (e.g., 1.0) to enable gradient clipping

  # --- VVVVV ADD LOSS WEIGHTS VVVVV ---
  loss_weights:
    seg: 1.0                  # Weight for segmentation CrossEntropyLoss
    silog: 0.1                # Weight for depth SILogLoss (adjust based on relative loss magnitudes)
    # l1: 0.0                   # Example: Set weight to 0 if not using L1 depth loss
  # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

  # (Optional: Add SILog loss parameters if different from defaults)
  # silog_loss:
  #   lambda: 0.5
  #   eps: 1e-6