# Model configuration
model:
  type: 'DenseCLIP'
  context_length: 77
  context_feature: 'attention'
  score_concat_index: 3
  text_head: False
  tau: 0.05  # Adjusted for better logit scaling
  token_embed_dim: 512
  text_dim: 1024
  clip_pretrained: '/home/22dcs005/DenseCLIP/segmentation/pretrained/RN50.pt' # Adjust path if needed based on where you run train_denseclip.py from

  backbone:
    type: 'CLIPResNet'
    layers: [3, 4, 6, 3]
    width: 64
    input_resolution: 512
    output_dim: 1024

  text_encoder:
    type: 'CLIPTextEncoder'
    context_length: 77
    vocab_size: 49408
    transformer_width: 512
    transformer_heads: 8
    transformer_layers: 12
    embed_dim: 1024

  neck:
    type: 'FPN'
    in_channels: [256, 512, 1024, 2048] # Channels from ResNet stages before projection
    out_channels: 256
    num_outs: 4 # Or 5 if using extra level

  decode_head:
    type: 'FPNHead'
    in_channels: 256
    channels: 256
    num_classes: 19
    align_corners: False
    dropout_ratio: 0.1

# Data configuration
data:
  dataset_type: 'CityscapesDataset'
  path: 'data/cityscapes'
  classes: 19
  ignore_label: 255
  crop_size: [512, 1024]
  scale_range: [0.5, 2.0]
  norm_mean: [0.48145466, 0.4578275, 0.40821073]
  norm_std: [0.26862954, 0.26130258, 0.27577711]
  samples_per_gpu: 16
  workers_per_gpu: 16

# Training configuration
training:
  epochs: 100
  batch_size: 16
  workers: 16
  optimizer:
    type: 'AdamW'  # Changed to AdamW
    lr: 0.00005  # Reduced learning rate for better stability
    weight_decay: 0.0001
  min_lr: 1.0e-06
  log_interval: 50
  seed: 42
  eval_interval: 1
  save_interval: 5
